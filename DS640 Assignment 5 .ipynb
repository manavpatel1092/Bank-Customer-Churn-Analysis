{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat customers are the most valuable customers you have.\n",
    "\n",
    "Repeat customers:\n",
    "\n",
    "- Are loyal and satisfied\n",
    "- Make multiple purchases\n",
    "- Talk about your brand\n",
    "- Refer your brand to their friends\n",
    "\n",
    "However, maintaining your relationship with loyal customers can be tricky.These relationships need to be maintained with a consistent and rewarding customer experience. When your customers are happy, your business will prosper.\n",
    "\n",
    "What is Churn ?\n",
    "Customers that have churned are customers that have cut ties with a business or brand.\n",
    "\n",
    "What is Churn Rate used for ?\n",
    "Churn rate is a measurement of the health of your business.\n",
    "Churn measures:\n",
    "\n",
    "- The success of increasing customer retention for a company\n",
    "- The changes that affect customer retention\n",
    "- Customer Lifetime Value\n",
    "- The type of customers that have the highest retention rate with your company\n",
    "- Much more!\n",
    "\n",
    "Today, we are going to analyse a customer dataset of a bank to provide them with valuable insights for their ongoing customer retention issue. We would apply the ANN(Artificial Neural Networks) to classify the dataset and provide the bank with the best fit model. The problem statement is mentioned in detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bank has been seen unusual churn rates for their customers (churn is when people leave the company). They want to understand the problem for this unusual high churn rates. Here we have been provided with a sample of their, last six months customers whose charecteristics like \n",
    "\n",
    "- credit score \n",
    "- geography\n",
    "- gender\n",
    "- age\n",
    "- tenure (how many years a customer is with the bank)\n",
    "- balance\n",
    "- number of products a customer had with the bank\n",
    "- if he/she had a credit card\n",
    "- if he/she is an active member \n",
    "- estimated salary (the bank has estimated the salary based of the data they had)\n",
    "- exited (if a customer has left the bank or not within the last six months)  \n",
    "\n",
    "Our task is to create a classification model for predicting customers at risk of churning. From this classification model the bank could gain useful insights and can take relevant actions to prevent further unusual churn rate and improve the overall customer experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprossesing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>15574012</td>\n",
       "      <td>Chu</td>\n",
       "      <td>645</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>15592531</td>\n",
       "      <td>Bartlett</td>\n",
       "      <td>822</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>15656148</td>\n",
       "      <td>Obinna</td>\n",
       "      <td>376</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>15792365</td>\n",
       "      <td>He</td>\n",
       "      <td>501</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>15592389</td>\n",
       "      <td>H?</td>\n",
       "      <td>684</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>134603.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71725.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>15767821</td>\n",
       "      <td>Bearce</td>\n",
       "      <td>528</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>102016.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80181.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>15737173</td>\n",
       "      <td>Andrews</td>\n",
       "      <td>497</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76390.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>15632264</td>\n",
       "      <td>Kay</td>\n",
       "      <td>476</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26260.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>15691483</td>\n",
       "      <td>Chin</td>\n",
       "      <td>549</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190857.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>15600882</td>\n",
       "      <td>Scott</td>\n",
       "      <td>635</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65951.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>15643966</td>\n",
       "      <td>Goforth</td>\n",
       "      <td>616</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>143129.41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64327.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>15737452</td>\n",
       "      <td>Romeo</td>\n",
       "      <td>653</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>132602.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5097.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>15788218</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>549</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14406.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>15661507</td>\n",
       "      <td>Muldrow</td>\n",
       "      <td>587</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158684.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>15568982</td>\n",
       "      <td>Hao</td>\n",
       "      <td>726</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54724.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>15577657</td>\n",
       "      <td>McDonald</td>\n",
       "      <td>732</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>170886.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>15597945</td>\n",
       "      <td>Dellucci</td>\n",
       "      <td>636</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138555.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>15699309</td>\n",
       "      <td>Gerasimov</td>\n",
       "      <td>510</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118913.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>15725737</td>\n",
       "      <td>Mosman</td>\n",
       "      <td>669</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8487.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>15625047</td>\n",
       "      <td>Yen</td>\n",
       "      <td>846</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>187616.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>15738191</td>\n",
       "      <td>Maclean</td>\n",
       "      <td>577</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>124508.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>15736816</td>\n",
       "      <td>Young</td>\n",
       "      <td>756</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>136815.64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>170041.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>15700772</td>\n",
       "      <td>Nebechi</td>\n",
       "      <td>571</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38433.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>15728693</td>\n",
       "      <td>McWilliams</td>\n",
       "      <td>574</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>141349.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100187.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>15656300</td>\n",
       "      <td>Lucciano</td>\n",
       "      <td>411</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>59697.17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53483.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>9971</td>\n",
       "      <td>15587133</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>518</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>151027.05</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119377.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>9972</td>\n",
       "      <td>15721377</td>\n",
       "      <td>Chou</td>\n",
       "      <td>833</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>144751.81</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166472.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>9973</td>\n",
       "      <td>15747927</td>\n",
       "      <td>Ch'in</td>\n",
       "      <td>758</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>155739.76</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>171552.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>9974</td>\n",
       "      <td>15806455</td>\n",
       "      <td>Miller</td>\n",
       "      <td>611</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>157474.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>9975</td>\n",
       "      <td>15695474</td>\n",
       "      <td>Barker</td>\n",
       "      <td>583</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>122531.86</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13549.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>9976</td>\n",
       "      <td>15666295</td>\n",
       "      <td>Smith</td>\n",
       "      <td>610</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>113957.01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>196526.55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>9977</td>\n",
       "      <td>15656062</td>\n",
       "      <td>Azikiwe</td>\n",
       "      <td>637</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>103377.81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84419.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>9978</td>\n",
       "      <td>15579969</td>\n",
       "      <td>Mancini</td>\n",
       "      <td>683</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24991.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>9979</td>\n",
       "      <td>15703563</td>\n",
       "      <td>P'eng</td>\n",
       "      <td>774</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>93017.47</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>191608.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>9980</td>\n",
       "      <td>15692664</td>\n",
       "      <td>Diribe</td>\n",
       "      <td>677</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>90022.85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2988.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>9981</td>\n",
       "      <td>15719276</td>\n",
       "      <td>T'ao</td>\n",
       "      <td>741</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>74371.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99595.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>9982</td>\n",
       "      <td>15672754</td>\n",
       "      <td>Burbidge</td>\n",
       "      <td>498</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>152039.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53445.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>9983</td>\n",
       "      <td>15768163</td>\n",
       "      <td>Griffin</td>\n",
       "      <td>655</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>137145.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>115146.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>9984</td>\n",
       "      <td>15656710</td>\n",
       "      <td>Cocci</td>\n",
       "      <td>613</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151325.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>9985</td>\n",
       "      <td>15696175</td>\n",
       "      <td>Echezonachukwu</td>\n",
       "      <td>602</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>90602.42</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51695.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>9986</td>\n",
       "      <td>15586914</td>\n",
       "      <td>Nepean</td>\n",
       "      <td>659</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>123841.49</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96833.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>9987</td>\n",
       "      <td>15581736</td>\n",
       "      <td>Bartlett</td>\n",
       "      <td>673</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>183579.54</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34047.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>9988</td>\n",
       "      <td>15588839</td>\n",
       "      <td>Mancini</td>\n",
       "      <td>606</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>180307.73</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1914.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>9989</td>\n",
       "      <td>15589329</td>\n",
       "      <td>Pirozzi</td>\n",
       "      <td>775</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49337.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>9990</td>\n",
       "      <td>15605622</td>\n",
       "      <td>McMillan</td>\n",
       "      <td>841</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>179436.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>9991</td>\n",
       "      <td>15798964</td>\n",
       "      <td>Nkemakonam</td>\n",
       "      <td>714</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>35016.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53667.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>9992</td>\n",
       "      <td>15769959</td>\n",
       "      <td>Ajuluchukwu</td>\n",
       "      <td>597</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>88381.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69384.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>9993</td>\n",
       "      <td>15657105</td>\n",
       "      <td>Chukwualuka</td>\n",
       "      <td>726</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>195192.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>9994</td>\n",
       "      <td>15569266</td>\n",
       "      <td>Rahman</td>\n",
       "      <td>644</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>155060.41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29179.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>9995</td>\n",
       "      <td>15719294</td>\n",
       "      <td>Wood</td>\n",
       "      <td>800</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167773.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RowNumber  CustomerId         Surname  CreditScore Geography  Gender  \\\n",
       "0             1    15634602        Hargrave          619    France  Female   \n",
       "1             2    15647311            Hill          608     Spain  Female   \n",
       "2             3    15619304            Onio          502    France  Female   \n",
       "3             4    15701354            Boni          699    France  Female   \n",
       "4             5    15737888        Mitchell          850     Spain  Female   \n",
       "5             6    15574012             Chu          645     Spain    Male   \n",
       "6             7    15592531        Bartlett          822    France    Male   \n",
       "7             8    15656148          Obinna          376   Germany  Female   \n",
       "8             9    15792365              He          501    France    Male   \n",
       "9            10    15592389              H?          684    France    Male   \n",
       "10           11    15767821          Bearce          528    France    Male   \n",
       "11           12    15737173         Andrews          497     Spain    Male   \n",
       "12           13    15632264             Kay          476    France  Female   \n",
       "13           14    15691483            Chin          549    France  Female   \n",
       "14           15    15600882           Scott          635     Spain  Female   \n",
       "15           16    15643966         Goforth          616   Germany    Male   \n",
       "16           17    15737452           Romeo          653   Germany    Male   \n",
       "17           18    15788218       Henderson          549     Spain  Female   \n",
       "18           19    15661507         Muldrow          587     Spain    Male   \n",
       "19           20    15568982             Hao          726    France  Female   \n",
       "20           21    15577657        McDonald          732    France    Male   \n",
       "21           22    15597945        Dellucci          636     Spain  Female   \n",
       "22           23    15699309       Gerasimov          510     Spain  Female   \n",
       "23           24    15725737          Mosman          669    France    Male   \n",
       "24           25    15625047             Yen          846    France  Female   \n",
       "25           26    15738191         Maclean          577    France    Male   \n",
       "26           27    15736816           Young          756   Germany    Male   \n",
       "27           28    15700772         Nebechi          571    France    Male   \n",
       "28           29    15728693      McWilliams          574   Germany  Female   \n",
       "29           30    15656300        Lucciano          411    France    Male   \n",
       "...         ...         ...             ...          ...       ...     ...   \n",
       "9970       9971    15587133        Thompson          518    France    Male   \n",
       "9971       9972    15721377            Chou          833    France  Female   \n",
       "9972       9973    15747927           Ch'in          758    France    Male   \n",
       "9973       9974    15806455          Miller          611    France    Male   \n",
       "9974       9975    15695474          Barker          583    France    Male   \n",
       "9975       9976    15666295           Smith          610   Germany    Male   \n",
       "9976       9977    15656062         Azikiwe          637    France  Female   \n",
       "9977       9978    15579969         Mancini          683    France  Female   \n",
       "9978       9979    15703563           P'eng          774    France    Male   \n",
       "9979       9980    15692664          Diribe          677    France  Female   \n",
       "9980       9981    15719276            T'ao          741     Spain    Male   \n",
       "9981       9982    15672754        Burbidge          498   Germany    Male   \n",
       "9982       9983    15768163         Griffin          655   Germany  Female   \n",
       "9983       9984    15656710           Cocci          613    France    Male   \n",
       "9984       9985    15696175  Echezonachukwu          602   Germany    Male   \n",
       "9985       9986    15586914          Nepean          659    France    Male   \n",
       "9986       9987    15581736        Bartlett          673   Germany    Male   \n",
       "9987       9988    15588839         Mancini          606     Spain    Male   \n",
       "9988       9989    15589329         Pirozzi          775    France    Male   \n",
       "9989       9990    15605622        McMillan          841     Spain    Male   \n",
       "9990       9991    15798964      Nkemakonam          714   Germany    Male   \n",
       "9991       9992    15769959     Ajuluchukwu          597    France  Female   \n",
       "9992       9993    15657105     Chukwualuka          726     Spain    Male   \n",
       "9993       9994    15569266          Rahman          644    France    Male   \n",
       "9994       9995    15719294            Wood          800    France  Female   \n",
       "9995       9996    15606229        Obijiaku          771    France    Male   \n",
       "9996       9997    15569892       Johnstone          516    France    Male   \n",
       "9997       9998    15584532             Liu          709    France  Female   \n",
       "9998       9999    15682355       Sabbatini          772   Germany    Male   \n",
       "9999      10000    15628319          Walker          792    France  Female   \n",
       "\n",
       "      Age  Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0      42       2       0.00              1          1               1   \n",
       "1      41       1   83807.86              1          0               1   \n",
       "2      42       8  159660.80              3          1               0   \n",
       "3      39       1       0.00              2          0               0   \n",
       "4      43       2  125510.82              1          1               1   \n",
       "5      44       8  113755.78              2          1               0   \n",
       "6      50       7       0.00              2          1               1   \n",
       "7      29       4  115046.74              4          1               0   \n",
       "8      44       4  142051.07              2          0               1   \n",
       "9      27       2  134603.88              1          1               1   \n",
       "10     31       6  102016.72              2          0               0   \n",
       "11     24       3       0.00              2          1               0   \n",
       "12     34      10       0.00              2          1               0   \n",
       "13     25       5       0.00              2          0               0   \n",
       "14     35       7       0.00              2          1               1   \n",
       "15     45       3  143129.41              2          0               1   \n",
       "16     58       1  132602.88              1          1               0   \n",
       "17     24       9       0.00              2          1               1   \n",
       "18     45       6       0.00              1          0               0   \n",
       "19     24       6       0.00              2          1               1   \n",
       "20     41       8       0.00              2          1               1   \n",
       "21     32       8       0.00              2          1               0   \n",
       "22     38       4       0.00              1          1               0   \n",
       "23     46       3       0.00              2          0               1   \n",
       "24     38       5       0.00              1          1               1   \n",
       "25     25       3       0.00              2          0               1   \n",
       "26     36       2  136815.64              1          1               1   \n",
       "27     44       9       0.00              2          0               0   \n",
       "28     43       3  141349.43              1          1               1   \n",
       "29     29       0   59697.17              2          1               1   \n",
       "...   ...     ...        ...            ...        ...             ...   \n",
       "9970   42       7  151027.05              2          1               0   \n",
       "9971   34       3  144751.81              1          0               0   \n",
       "9972   26       4  155739.76              1          1               0   \n",
       "9973   27       7       0.00              2          1               1   \n",
       "9974   33       7  122531.86              1          1               0   \n",
       "9975   50       1  113957.01              2          1               0   \n",
       "9976   33       7  103377.81              1          1               0   \n",
       "9977   32       9       0.00              2          1               1   \n",
       "9978   40       9   93017.47              2          1               0   \n",
       "9979   58       1   90022.85              1          0               1   \n",
       "9980   35       6   74371.49              1          0               0   \n",
       "9981   42       3  152039.70              1          1               1   \n",
       "9982   46       7  137145.12              1          1               0   \n",
       "9983   40       4       0.00              1          0               0   \n",
       "9984   35       7   90602.42              2          1               1   \n",
       "9985   36       6  123841.49              2          1               0   \n",
       "9986   47       1  183579.54              2          0               1   \n",
       "9987   30       8  180307.73              2          1               1   \n",
       "9988   30       4       0.00              2          1               0   \n",
       "9989   28       4       0.00              2          1               1   \n",
       "9990   33       3   35016.60              1          1               0   \n",
       "9991   53       4   88381.21              1          1               0   \n",
       "9992   36       2       0.00              1          1               0   \n",
       "9993   28       7  155060.41              1          1               0   \n",
       "9994   29       2       0.00              2          0               0   \n",
       "9995   39       5       0.00              2          1               0   \n",
       "9996   35      10   57369.61              1          1               1   \n",
       "9997   36       7       0.00              1          0               1   \n",
       "9998   42       3   75075.31              2          1               0   \n",
       "9999   28       4  130142.79              1          1               0   \n",
       "\n",
       "      EstimatedSalary  Exited  \n",
       "0           101348.88       1  \n",
       "1           112542.58       0  \n",
       "2           113931.57       1  \n",
       "3            93826.63       0  \n",
       "4            79084.10       0  \n",
       "5           149756.71       1  \n",
       "6            10062.80       0  \n",
       "7           119346.88       1  \n",
       "8            74940.50       0  \n",
       "9            71725.73       0  \n",
       "10           80181.12       0  \n",
       "11           76390.01       0  \n",
       "12           26260.98       0  \n",
       "13          190857.79       0  \n",
       "14           65951.65       0  \n",
       "15           64327.26       0  \n",
       "16            5097.67       1  \n",
       "17           14406.41       0  \n",
       "18          158684.81       0  \n",
       "19           54724.03       0  \n",
       "20          170886.17       0  \n",
       "21          138555.46       0  \n",
       "22          118913.53       1  \n",
       "23            8487.75       0  \n",
       "24          187616.16       0  \n",
       "25          124508.29       0  \n",
       "26          170041.95       0  \n",
       "27           38433.35       0  \n",
       "28          100187.43       0  \n",
       "29           53483.21       0  \n",
       "...               ...     ...  \n",
       "9970        119377.36       0  \n",
       "9971        166472.81       0  \n",
       "9972        171552.02       0  \n",
       "9973        157474.10       0  \n",
       "9974         13549.24       0  \n",
       "9975        196526.55       1  \n",
       "9976         84419.78       0  \n",
       "9977         24991.92       0  \n",
       "9978        191608.97       0  \n",
       "9979          2988.28       0  \n",
       "9980         99595.67       0  \n",
       "9981         53445.17       1  \n",
       "9982        115146.40       1  \n",
       "9983        151325.24       0  \n",
       "9984         51695.41       0  \n",
       "9985         96833.00       0  \n",
       "9986         34047.54       0  \n",
       "9987          1914.41       0  \n",
       "9988         49337.84       0  \n",
       "9989        179436.60       0  \n",
       "9990         53667.08       0  \n",
       "9991         69384.71       1  \n",
       "9992        195192.40       0  \n",
       "9993         29179.52       0  \n",
       "9994        167773.55       0  \n",
       "9995         96270.64       0  \n",
       "9996        101699.77       0  \n",
       "9997         42085.58       1  \n",
       "9998         92888.52       1  \n",
       "9999         38190.78       0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the dataset\n",
    "dataset = pd.read_csv('/Users/Lenovo/Desktop/Churn_Modelling.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569094e+07</td>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>5.012800</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.70550</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.89568</td>\n",
       "      <td>7.193619e+04</td>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.892174</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.45584</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.556570e+07</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2500.75000</td>\n",
       "      <td>1.562853e+07</td>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569074e+07</td>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7500.25000</td>\n",
       "      <td>1.575323e+07</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.581569e+07</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RowNumber    CustomerId   CreditScore           Age        Tenure  \\\n",
       "count  10000.00000  1.000000e+04  10000.000000  10000.000000  10000.000000   \n",
       "mean    5000.50000  1.569094e+07    650.528800     38.921800      5.012800   \n",
       "std     2886.89568  7.193619e+04     96.653299     10.487806      2.892174   \n",
       "min        1.00000  1.556570e+07    350.000000     18.000000      0.000000   \n",
       "25%     2500.75000  1.562853e+07    584.000000     32.000000      3.000000   \n",
       "50%     5000.50000  1.569074e+07    652.000000     37.000000      5.000000   \n",
       "75%     7500.25000  1.575323e+07    718.000000     44.000000      7.000000   \n",
       "max    10000.00000  1.581569e+07    850.000000     92.000000     10.000000   \n",
       "\n",
       "             Balance  NumOfProducts    HasCrCard  IsActiveMember  \\\n",
       "count   10000.000000   10000.000000  10000.00000    10000.000000   \n",
       "mean    76485.889288       1.530200      0.70550        0.515100   \n",
       "std     62397.405202       0.581654      0.45584        0.499797   \n",
       "min         0.000000       1.000000      0.00000        0.000000   \n",
       "25%         0.000000       1.000000      0.00000        0.000000   \n",
       "50%     97198.540000       1.000000      1.00000        1.000000   \n",
       "75%    127644.240000       2.000000      1.00000        1.000000   \n",
       "max    250898.090000       4.000000      1.00000        1.000000   \n",
       "\n",
       "       EstimatedSalary        Exited  \n",
       "count     10000.000000  10000.000000  \n",
       "mean     100090.239881      0.203700  \n",
       "std       57510.492818      0.402769  \n",
       "min          11.580000      0.000000  \n",
       "25%       51002.110000      0.000000  \n",
       "50%      100193.915000      0.000000  \n",
       "75%      149388.247500      0.000000  \n",
       "max      199992.480000      1.000000  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary of the dataset \n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that their is no missing value in the dataset as the size of the dataset is 10000x14 and all the attributes have a count of 10000.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not consider the row number, the customer id and the surname as independent variables as they would have no impact on our dependent variable. Hence we trim the dataset and divide the dataset as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 3:13].values #Feature Variable \n",
    "y = dataset.iloc[:, 13].values #Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n",
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model needs in input numerical data, so, we need to encode categorical data into numerical data. In This case we have Geography (France, Spain and Germany) and Gender (Male and Female). For Geography we will have 0,1,2 instead of France, Spain and Gemany and 0,1 instead of Gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to Variables \n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a binary value for the column 'Gender' hence we don't have issues with the encoding. \n",
    "\n",
    "But, for the column 'Geography', we have 3 possible values and considering that our categorical values have no comparison (for example, Germany is not higher than France), we need to create dummy variables, so we will create 3 columns, one for each country where we will have 1 if the customer is from that country and 0 if not. And to avoid the dummy variable trap, we will also delete 1 of the 3 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dummy variable \n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into Training and testing sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply feature scaling to the dataset to standardize the dataset to convert them into same sclae for faster computation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature scaling \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the ANN with one hidden layer and having 6 nodes (Half of feature and target variables) with activation function of the hidden and the input layer as 'relu' (rectified linear units) and the output layer 'sigmoid' as we need binary result for our classification model. Then we compile the model with SGD 'adam' as its the best which can be used in thos case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#Creating a Neural network \n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))#The Input layer \n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))#The hidden layer 1\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))#The Output layer \n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])#Compiling the result with SGD as adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 4s 499us/step - loss: 0.4931 - acc: 0.7954\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 2s 265us/step - loss: 0.4302 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 2s 255us/step - loss: 0.4255 - acc: 0.7960\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.4210 - acc: 0.7999\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 0.4182 - acc: 0.8226\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.4161 - acc: 0.8262\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.4144 - acc: 0.8294\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.4132 - acc: 0.8292\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4125 - acc: 0.8312\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 0.4113 - acc: 0.8317\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 2s 280us/step - loss: 0.4103 - acc: 0.8336\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 0.4094 - acc: 0.8326\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.4088 - acc: 0.8324\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.4086 - acc: 0.8350\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4079 - acc: 0.8349\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4074 - acc: 0.8335\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.4068 - acc: 0.8341\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.4065 - acc: 0.8340\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 2s 273us/step - loss: 0.4059 - acc: 0.8341\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 2s 285us/step - loss: 0.4056 - acc: 0.8346\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 2s 256us/step - loss: 0.4056 - acc: 0.8347\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 0.4049 - acc: 0.8350\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 186us/step - loss: 0.4047 - acc: 0.8325\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.4045 - acc: 0.8350\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 0.4039 - acc: 0.8336\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.4038 - acc: 0.8365\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.4038 - acc: 0.8352\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 0.4033 - acc: 0.8342\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 2s 271us/step - loss: 0.4038 - acc: 0.8341\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 2s 261us/step - loss: 0.4034 - acc: 0.8345\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 0.4031 - acc: 0.8354\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.4027 - acc: 0.8346\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.4028 - acc: 0.8371\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.4024 - acc: 0.8341\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.4019 - acc: 0.8336\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4019 - acc: 0.8347\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 0.4022 - acc: 0.8360\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 0.4019 - acc: 0.8345\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 0.4020 - acc: 0.8372\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.4025 - acc: 0.8351\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 0.4018 - acc: 0.8361\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.4015 - acc: 0.8337\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.4019 - acc: 0.8360\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 2s 255us/step - loss: 0.4017 - acc: 0.8344\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 0.4015 - acc: 0.8350\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.4017 - acc: 0.8334\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 254us/step - loss: 0.4017 - acc: 0.8349\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 250us/step - loss: 0.4012 - acc: 0.8362\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 0.4015 - acc: 0.8365\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 258us/step - loss: 0.4013 - acc: 0.8345\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 0.4016 - acc: 0.8349\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.4012 - acc: 0.8360\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.4009 - acc: 0.8354\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.4011 - acc: 0.8346\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 254us/step - loss: 0.4011 - acc: 0.8342\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 250us/step - loss: 0.4009 - acc: 0.8351\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 0.4011 - acc: 0.8342\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.4005 - acc: 0.8350\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.4004 - acc: 0.8339\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 0.4008 - acc: 0.8330\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 0.4010 - acc: 0.8346\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.4007 - acc: 0.8347\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.4005 - acc: 0.8346\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.4007 - acc: 0.8354\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.4008 - acc: 0.8344\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 0.4010 - acc: 0.8345\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.4003 - acc: 0.8349\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.4003 - acc: 0.8342\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.4005 - acc: 0.8359\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.4006 - acc: 0.8346\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.4001 - acc: 0.8349\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.4003 - acc: 0.8347\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 2s 256us/step - loss: 0.4005 - acc: 0.8359\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 0.4004 - acc: 0.8340\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 0.4007 - acc: 0.8342\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 0.4005 - acc: 0.8361\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.3999 - acc: 0.8357\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 0.4004 - acc: 0.8345 1s - loss: 0.\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 0.4000 - acc: 0.8342\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 0.4003 - acc: 0.8364\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 0.4005 - acc: 0.8345\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 199us/step - loss: 0.4003 - acc: 0.8362\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.4003 - acc: 0.8349\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 0.4002 - acc: 0.8355\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.3999 - acc: 0.8346\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.4000 - acc: 0.8341\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 0.3997 - acc: 0.8364 0s - loss:\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 0.4002 - acc: 0.8355\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.3996 - acc: 0.8359\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 0.3996 - acc: 0.8342\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.3996 - acc: 0.8336\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 2s 238us/step - loss: 0.3998 - acc: 0.8351\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 0.3999 - acc: 0.8364\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 254us/step - loss: 0.3999 - acc: 0.8351\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.3998 - acc: 0.8365\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.3997 - acc: 0.8369\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.3995 - acc: 0.8342\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.3995 - acc: 0.8359\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 0.3992 - acc: 0.8347\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 0.3995 - acc: 0.8369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a082447278>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the Neural network model on the testing data \n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the testing set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1539   56]\n",
      " [ 256  149]]\n"
     ]
    }
   ],
   "source": [
    "#Creating the confusion matrix \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the accuracy of the model\n",
    "accuracy_ANN = (cm[0,0]+cm[1,1])/ (cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])\n",
    "accuracy_ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Specificity ?\n",
    "\n",
    "Specificity (also called the true negative rate) measures the proportion of actual negatives that are correctly identified.\n",
    "Specificity relates to the test's ability to correctly reject customers who did not churn and actually did'nt churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7268292682926829"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the specificity of the model\n",
    "specificity_ANN =(cm[1,1]/(cm[1,1]+cm[0,1]))\n",
    "specificity_ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Sensitivity ?\n",
    "\n",
    "Sensitivity (also called the true positive rate) measures the proportion of actual positives that are correctly identified. Sensitivity refers to the test's ability to correctly detect people who are going to churn and actually do churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8573816155988858"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the sensitivity of the model\n",
    "sensitivity_ANN =(cm[0,0]/(cm[0,0]+cm[1,0]))\n",
    "sensitivity_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 38us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17338521599769594, 1.0]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss function \n",
    "classifier_1.evaluate(X_test,y_pred1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the ANN with one hidden layer and having 6 nodes (Half of feature and target variables) with activation function of the hidden and the input layer as 'tanh' (hyperbolic tangent) and the output layer 'sigmoid' as we need binary result for our classification model. Then we compile the model with SGD 'adam' as its the best which can be used in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", units=6, kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#Creating a Neural Network model \n",
    "classifier_1 = Sequential()\n",
    "classifier_1.add(Dense(output_dim = 6, init = 'uniform', activation = 'tanh', input_dim = 11))\n",
    "classifier_1.add(Dense(output_dim = 6, init = 'uniform', activation = 'tanh'))\n",
    "classifier_1.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "classifier_1.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 3s 344us/step - loss: 0.4769 - acc: 0.8085\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.4335 - acc: 0.8121\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 0.4322 - acc: 0.8102\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 2s 258us/step - loss: 0.4293 - acc: 0.8116\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.4202 - acc: 0.8144\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.4023 - acc: 0.8306\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.3820 - acc: 0.8391\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.3660 - acc: 0.8510\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.3565 - acc: 0.8545\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 0.3520 - acc: 0.8579\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 0.3489 - acc: 0.8584\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 2s 263us/step - loss: 0.3490 - acc: 0.8579\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 2s 259us/step - loss: 0.3475 - acc: 0.8599\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.3476 - acc: 0.8584\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.3464 - acc: 0.8570\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 0.3460 - acc: 0.8584\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.3458 - acc: 0.8584\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.3454 - acc: 0.8605\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.3449 - acc: 0.8604\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 0.3449 - acc: 0.8600\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 0.3447 - acc: 0.8591\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 2s 261us/step - loss: 0.3443 - acc: 0.8565\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.3431 - acc: 0.8580\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.3432 - acc: 0.8571\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.3442 - acc: 0.8586\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.3427 - acc: 0.8589\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 0.3423 - acc: 0.8577\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 0.3426 - acc: 0.8579\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 2s 284us/step - loss: 0.3425 - acc: 0.8592\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 2s 260us/step - loss: 0.3418 - acc: 0.8569\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 0.3410 - acc: 0.8582\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.3403 - acc: 0.8587\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.3399 - acc: 0.8604\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.3393 - acc: 0.8619\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.3394 - acc: 0.8599\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.3387 - acc: 0.8601\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 0.3378 - acc: 0.8590\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 0.3385 - acc: 0.8607\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 2s 274us/step - loss: 0.3370 - acc: 0.8591\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 0.3369 - acc: 0.8605\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 0.3372 - acc: 0.8595\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 0.3358 - acc: 0.8612\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.3372 - acc: 0.8602\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.3361 - acc: 0.8622\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 2s 258us/step - loss: 0.3365 - acc: 0.8619\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 274us/step - loss: 0.3357 - acc: 0.8614\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 0.3360 - acc: 0.8605\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.3352 - acc: 0.8605 1s - loss: 0. - ETA: 0s - loss: 0.3\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 0.3357 - acc: 0.8614\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.3350 - acc: 0.8632\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.3358 - acc: 0.8619\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 0.3353 - acc: 0.8616\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 0.3348 - acc: 0.8614\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 261us/step - loss: 0.3348 - acc: 0.8635\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 255us/step - loss: 0.3351 - acc: 0.8624\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.3344 - acc: 0.8621\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.3346 - acc: 0.8624\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.3337 - acc: 0.8602\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.3327 - acc: 0.8625\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.3338 - acc: 0.8624\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.3339 - acc: 0.8624\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 0.3335 - acc: 0.8614\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 0.3332 - acc: 0.8642\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 0.3335 - acc: 0.8627\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.3328 - acc: 0.8646\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.3332 - acc: 0.8621\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.3332 - acc: 0.8635\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.3332 - acc: 0.8630\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.3320 - acc: 0.8640\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 0.3330 - acc: 0.8626\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 2s 260us/step - loss: 0.3327 - acc: 0.8644\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 0.3328 - acc: 0.8612\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.3330 - acc: 0.8621\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 0.3321 - acc: 0.8619\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.3326 - acc: 0.8619\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.3328 - acc: 0.8630\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.3327 - acc: 0.8631\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.3317 - acc: 0.8634\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 0.3326 - acc: 0.8651\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 0.3325 - acc: 0.8631\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 247us/step - loss: 0.3327 - acc: 0.8611\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.3325 - acc: 0.8639\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.3328 - acc: 0.8631\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.3318 - acc: 0.8665\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 0.3328 - acc: 0.8631\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.3323 - acc: 0.8635\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3325 - acc: 0.8637\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 0.3321 - acc: 0.8654\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 0.3328 - acc: 0.8629\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 0.3326 - acc: 0.8630\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 0.3317 - acc: 0.8647\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.3325 - acc: 0.8615\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.3319 - acc: 0.8651\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.3323 - acc: 0.8646\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.3324 - acc: 0.8630\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.3325 - acc: 0.8647\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 0.3318 - acc: 0.8632\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 2s 260us/step - loss: 0.3326 - acc: 0.8620\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.3320 - acc: 0.8631\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.3321 - acc: 0.8635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a0821e3ef0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the model on testing data \n",
    "classifier_1.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the target values using the model on testing data \n",
    "y_pred1 = classifier_1.predict(X_test)\n",
    "y_pred1 = (y_pred1 > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1513   82]\n",
      " [ 194  211]]\n"
     ]
    }
   ],
   "source": [
    "#Creating the confusion matrix \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm1 = confusion_matrix(y_test, y_pred1)\n",
    "print(cm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.862"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the accuracy of the model\n",
    "accuracy_ANN1 = (cm1[0,0]+cm1[1,1])/ (cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
    "accuracy_ANN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7201365187713311"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the specificity of the model\n",
    "specificity_ANN1 =(cm1[1,1]/(cm1[1,1]+cm1[0,1]))\n",
    "specificity_ANN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8863503222026948"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the sensitivity of the model\n",
    "sensitivity_ANN1 =(cm1[0,0]/(cm1[0,0]+cm1[1,0]))\n",
    "sensitivity_ANN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 181us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17338521599769594, 1.0]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the loss function of the model \n",
    "classifier_1.evaluate(X_test,y_pred1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the ANN with two hidden layer and having 6 nodes (Half of feature and target variables) with activation function of the hidden and the input layers as 'tanh' (hyperbolic tangent) and the output layer 'sigmoid' as we need binary result for our classification model. Then we compile the model with SGD 'adam' as its the best which can be used in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", units=6, kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", units=6, kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 8s 986us/step - loss: 0.4787 - acc: 0.8046\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 2s 272us/step - loss: 0.4353 - acc: 0.8164\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.4341 - acc: 0.8129\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.4310 - acc: 0.8129\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.4257 - acc: 0.8171\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.4179 - acc: 0.8239\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 0.4103 - acc: 0.8301\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.3993 - acc: 0.8360\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 2s 286us/step - loss: 0.3805 - acc: 0.8436\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 2s 285us/step - loss: 0.3644 - acc: 0.8511\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 2s 258us/step - loss: 0.3541 - acc: 0.8560\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 0.3523 - acc: 0.8549\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 0.3511 - acc: 0.8590\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 0.3488 - acc: 0.8582\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 0.3490 - acc: 0.8596\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 2s 250us/step - loss: 0.3478 - acc: 0.8609\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 2s 281us/step - loss: 0.3477 - acc: 0.8587\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 2s 281us/step - loss: 0.3478 - acc: 0.8606\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 0.3477 - acc: 0.8565\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 0.3464 - acc: 0.8601\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.3467 - acc: 0.8597\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.3468 - acc: 0.8610\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 2s 238us/step - loss: 0.3459 - acc: 0.8594\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 0.3455 - acc: 0.8589\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 2s 287us/step - loss: 0.3454 - acc: 0.8595\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 2s 280us/step - loss: 0.3451 - acc: 0.8595\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 0.3434 - acc: 0.8612\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 0.3442 - acc: 0.8605\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.3438 - acc: 0.8622\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 0.3438 - acc: 0.8587\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 0.3450 - acc: 0.8587\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 282us/step - loss: 0.3440 - acc: 0.8586\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 2s 279us/step - loss: 0.3434 - acc: 0.8622\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 267us/step - loss: 0.3424 - acc: 0.8630\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 0.3435 - acc: 0.8596\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 0.3420 - acc: 0.8607\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 0.3441 - acc: 0.8615\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.3432 - acc: 0.8626\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 0.3428 - acc: 0.8621\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 0.3423 - acc: 0.8614\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 3s 316us/step - loss: 0.3427 - acc: 0.8627\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 2s 292us/step - loss: 0.3423 - acc: 0.8635\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.3415 - acc: 0.8610 1s - loss:\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.3429 - acc: 0.8612\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.3418 - acc: 0.861 - 2s 251us/step - loss: 0.3420 - acc: 0.8619\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 272us/step - loss: 0.3417 - acc: 0.8609\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.3417 - acc: 0.8617\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 274us/step - loss: 0.3422 - acc: 0.8637\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.3419 - acc: 0.8610\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 0.3421 - acc: 0.8605\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3401 - acc: 0.8620\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3404 - acc: 0.8625\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 0.3415 - acc: 0.8616\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 262us/step - loss: 0.3407 - acc: 0.8614\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.3407 - acc: 0.8615\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.3405 - acc: 0.8611\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: 0.3402 - acc: 0.8612\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 279us/step - loss: 0.3404 - acc: 0.8617\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 255us/step - loss: 0.3397 - acc: 0.8641\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.3392 - acc: 0.8611\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 0.3393 - acc: 0.8634\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.3395 - acc: 0.8637\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.3382 - acc: 0.8637\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 0.3382 - acc: 0.8639\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 2s 265us/step - loss: 0.3381 - acc: 0.8616\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 0.3382 - acc: 0.8605\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 250us/step - loss: 0.3378 - acc: 0.8617\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 0.3379 - acc: 0.8611\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 0.3374 - acc: 0.8636\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 0.3368 - acc: 0.8611\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.3369 - acc: 0.8610\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 0.3368 - acc: 0.8611\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 3s 342us/step - loss: 0.3365 - acc: 0.8627\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 0.3356 - acc: 0.8620\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 2s 258us/step - loss: 0.3364 - acc: 0.8607\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 0.3361 - acc: 0.8615\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.3354 - acc: 0.8639\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.3355 - acc: 0.8629\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.3349 - acc: 0.8635\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.3351 - acc: 0.8617\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.3348 - acc: 0.8627\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 0.3348 - acc: 0.8627\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 0.3355 - acc: 0.8626\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 0.3353 - acc: 0.8631\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.3339 - acc: 0.8641\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 0.3343 - acc: 0.8652\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.3348 - acc: 0.8634\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.3331 - acc: 0.8636\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 2s 254us/step - loss: 0.3347 - acc: 0.8627\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 2s 281us/step - loss: 0.3341 - acc: 0.8651\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 2s 262us/step - loss: 0.3330 - acc: 0.8656\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.3335 - acc: 0.8656\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.3340 - acc: 0.8642\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.3338 - acc: 0.8626\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.3341 - acc: 0.8615\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 0.3336 - acc: 0.8626\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 0.3326 - acc: 0.8641\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 2s 296us/step - loss: 0.3334 - acc: 0.8636\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 2s 271us/step - loss: 0.3333 - acc: 0.8626\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.3321 - acc: 0.8654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a083c03d68>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a Neural Network model \n",
    "classifier_2 = Sequential()\n",
    "classifier_2.add(Dense(output_dim = 6, init = 'uniform', activation = 'tanh', input_dim = 11))\n",
    "classifier_2.add(Dense(output_dim = 6, init = 'uniform', activation = 'tanh'))\n",
    "classifier_2.add(Dense(output_dim = 6, init = 'uniform', activation = 'tanh'))\n",
    "classifier_2.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "classifier_2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier_2.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting the model with test set \n",
    "y_pred2 = classifier_2.predict(X_test)\n",
    "y_pred2 = (y_pred2 > 0.5)\n",
    "y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1521   74]\n",
      " [ 203  202]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8615"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the confusion matrix \n",
    "cm2 = confusion_matrix(y_test, y_pred2)\n",
    "print(cm2)\n",
    "\n",
    "#Accuracy of the model \n",
    "accuracy_ANN2 = (cm2[0,0]+cm2[1,1])/ (cm2[0,0]+cm2[0,1]+cm2[1,0]+cm2[1,1])\n",
    "accuracy_ANN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7318840579710145"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the specificity of the model\n",
    "specificity_ANN2 =(cm2[1,1]/(cm2[1,1]+cm2[0,1]))\n",
    "specificity_ANN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8822505800464037"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the sensitivity of the model\n",
    "sensitivity_ANN2 =(cm2[0,0]/(cm2[0,0]+cm2[1,0]))\n",
    "sensitivity_ANN2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 3s 393us/step - loss: 0.4793 - acc: 0.7941\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 0.4314 - acc: 0.7951\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 2s 261us/step - loss: 0.4301 - acc: 0.7974\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 0.4278 - acc: 0.8022\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 2s 270us/step - loss: 0.4226 - acc: 0.8131\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 282us/step - loss: 0.4153 - acc: 0.8211\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 2s 273us/step - loss: 0.4006 - acc: 0.8326\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 0.3702 - acc: 0.8511\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 0.3553 - acc: 0.8579\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 0.3501 - acc: 0.8595\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 2s 281us/step - loss: 0.3487 - acc: 0.8587\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 0.3475 - acc: 0.8596\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 3s 317us/step - loss: 0.3459 - acc: 0.8595\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 2s 290us/step - loss: 0.3461 - acc: 0.8600\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 2s 270us/step - loss: 0.3453 - acc: 0.8584\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 0.3451 - acc: 0.8594\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.3444 - acc: 0.8605\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 0.3437 - acc: 0.8602\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 0.3437 - acc: 0.8610\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 2s 256us/step - loss: 0.3439 - acc: 0.8620\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 0.3420 - acc: 0.8625\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 2s 273us/step - loss: 0.3426 - acc: 0.8615\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.3424 - acc: 0.8605\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 2s 238us/step - loss: 0.3410 - acc: 0.8605\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 0.3411 - acc: 0.8624\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 0.3408 - acc: 0.8607\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 2s 238us/step - loss: 0.3400 - acc: 0.8610\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 0.3397 - acc: 0.8610\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 2s 282us/step - loss: 0.3400 - acc: 0.8587\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 0.3392 - acc: 0.8614\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 0.3390 - acc: 0.8630\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 0.3389 - acc: 0.8600\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 0.3388 - acc: 0.8635\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 0.3381 - acc: 0.8601\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 0.3372 - acc: 0.8627\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 2s 287us/step - loss: 0.3379 - acc: 0.8621\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 2s 282us/step - loss: 0.3378 - acc: 0.8626\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 0.3372 - acc: 0.8605\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 0.3377 - acc: 0.8626\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 2s 253us/step - loss: 0.3362 - acc: 0.8621\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 0.3370 - acc: 0.8614\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 0.3366 - acc: 0.8634\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 0.3356 - acc: 0.8640\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 2s 283us/step - loss: 0.3362 - acc: 0.8621\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 0.3358 - acc: 0.8616\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 260us/step - loss: 0.3351 - acc: 0.8639\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 0.3358 - acc: 0.8631\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 254us/step - loss: 0.3359 - acc: 0.8611\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 0.3357 - acc: 0.8620\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 0.3354 - acc: 0.8635\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 0.3356 - acc: 0.8631\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 282us/step - loss: 0.3354 - acc: 0.8619\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 273us/step - loss: 0.3344 - acc: 0.8624\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 0.3348 - acc: 0.8619\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 0.3347 - acc: 0.8634\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 0.3345 - acc: 0.8636\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 0.3345 - acc: 0.8616\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 0.3347 - acc: 0.8615\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 253us/step - loss: 0.3334 - acc: 0.8634\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 280us/step - loss: 0.3343 - acc: 0.8612\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 2s 273us/step - loss: 0.3345 - acc: 0.8622\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 0.3341 - acc: 0.8612\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 0.3342 - acc: 0.8619\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 0.3337 - acc: 0.8642\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 0.3334 - acc: 0.8614\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 0.3332 - acc: 0.8625\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 0.3333 - acc: 0.8655\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 0.3338 - acc: 0.8630\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 281us/step - loss: 0.3338 - acc: 0.8626\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 0.3336 - acc: 0.8637\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.3338 - acc: 0.8625\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 0.3333 - acc: 0.8637\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 0.3333 - acc: 0.8640\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 0.3335 - acc: 0.8612\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 0.3336 - acc: 0.8609\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 0.3329 - acc: 0.8624\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 0.3337 - acc: 0.8619\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 0.3327 - acc: 0.8626\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 0.3331 - acc: 0.8629\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.3328 - acc: 0.8644\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 2s 253us/step - loss: 0.3325 - acc: 0.8615\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 274us/step - loss: 0.3333 - acc: 0.8634\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 2s 253us/step - loss: 0.3327 - acc: 0.8639\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 2s 274us/step - loss: 0.3334 - acc: 0.8640\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 2s 281us/step - loss: 0.3327 - acc: 0.8632\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 0.3328 - acc: 0.8625\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.3323 - acc: 0.8634\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.3324 - acc: 0.8647\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 0.3317 - acc: 0.8646\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 0.3325 - acc: 0.8630\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 2s 263us/step - loss: 0.3322 - acc: 0.8656\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 2s 297us/step - loss: 0.3325 - acc: 0.8635\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: 0.3319 - acc: 0.8622\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 0.3319 - acc: 0.8627\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 0.3323 - acc: 0.8612\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 0.3322 - acc: 0.8630\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 0.3324 - acc: 0.8632\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 0.3316 - acc: 0.8630\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 0.3316 - acc: 0.8636\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 0.3314 - acc: 0.8636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a084107a20>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the Neural Network model \n",
    "classifier_2 = Sequential()\n",
    "classifier_2.add(Dense(output_dim = 6, init = 'uniform', activation = 'tanh', input_dim = 11))\n",
    "classifier_2.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "classifier_2.add(Dense(output_dim = 6, init = 'uniform', activation = 'tanh'))\n",
    "classifier_2.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "classifier_2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#Fitting the model on testing data \n",
    "classifier_2.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the target value using the model on testing data \n",
    "y_pred3 = classifier_2.predict(X_test)\n",
    "y_pred3 = (y_pred3 > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1515   80]\n",
      " [ 201  204]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8595"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the confusion matrix \n",
    "cm3 = confusion_matrix(y_test, y_pred3)\n",
    "print(cm3)\n",
    "#Calculating the accuracy of the model \n",
    "accuracy_ANN3 = (cm3[0,0]+cm3[1,1])/ (cm3[0,0]+cm3[0,1]+cm3[1,0]+cm3[1,1])\n",
    "accuracy_ANN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7183098591549296"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the Specificity of the model  \n",
    "specificity_ANN3 =(cm3[1,1]/(cm3[1,1]+cm3[0,1]))\n",
    "specificity_ANN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8828671328671329"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Calculating the Sensitivity of the model\n",
    "sensitivity_ANN3 =(cm3[0,0]/(cm3[0,0]+cm3[1,0]))\n",
    "sensitivity_ANN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model 2 has the accurate representation of the churn rate of bank customers as we achieved highest accuracy among all the ANN models with varying activation functions and number of hidden layers. It has better accuracy of 86.2% with specificity 72.01% and 88.6% sensitivity and low loss function of 0.173.\n",
    "\n",
    "The model 2 can provide most accurate classification to the bank for their customers at risk of churning. From that data the bank can analyse the customer's requirement and problems to cater the customers in a better manner and to avoid churn.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
